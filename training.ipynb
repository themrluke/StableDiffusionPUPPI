{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 1\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "from data_processing import Dataset\n",
    "from noise import NoiseScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms  #provides common image transformations for data preprocessing\n",
    "import torch    #used for tensor computations and building neural networks\n",
    "from torch.utils.data import TensorDataset, DataLoader  #TensorDataset wraps tensors. DataLoader creates iterable dataloaders for datasets.\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup # This function creates a learning rate scheduler with a cosine decay and warmup period\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm # Library used to display progress bars for loops, making it easy to track the progress of an iteration\n",
    "from torch.optim import Adam # Popular optimization algorithm used in training neural networks.\n",
    "from pathlib import Path # Object-oriented interface for filesystem paths.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"Datasets\"       # Set to directory where data is stored\n",
    "\n",
    "num_events = 10000 # Adjust number of events to train model here\n",
    "start_idx = 0\n",
    "end_idx = num_events\n",
    "dataset = Dataset(num_events, (120, 72), signal_file=f\"{data_dir}/CaloImages_signal.root\", pile_up_file=f\"{data_dir}/CaloImages_bkg.root\", save=False, start_idx=start_idx, end_idx=end_idx) # Can set to 10000\n",
    "# 1000: number of samples in dataset\n",
    "# (120, 72): Shape of each data sample (eg. image with dimensions 120x72)\n",
    "# signal_file: Signal file for the dataset\n",
    "# pile_up_file: This file contains background/ pileup data for the dataset\n",
    "# save=False means the dataset should not be saved to disk after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading file Datasets/CaloImages_signal.root\n",
      "loading file Datasets/CaloImages_signal.root\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1134.32it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1193.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset() # once this is cached, you don't have to re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim=(64,64) #resize each data sample image into 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:re-sizing\n",
      "re-sizing\n"
     ]
    }
   ],
   "source": [
    "saturation_value = 512 # Change saturation energy here\n",
    "dataset.preprocess(saturation_value, new_dim)\n",
    "# Pixels with an energy greater than the first number (eg.16 or 64 etc) will be clipped and shown as this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from image from dataset from original format into PyTorch tensor\n",
    "preprocess = transforms.Compose( # Chain together multiple image transformations. The transformations are applied sequentially in the order they are specified within the list.\n",
    "        [   \n",
    "            transforms.ToTensor()   # This is a transformation that converts an image (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "                                    # H stands for height, W stands for width, and C stands for the number of channels (e.g., 3 for RGB images).\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOVING DATA TO GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract horizontal strip from y=26 to y=38 (12 pixels tall)\n",
    "# Change how much of image to train model on here\n",
    "strip_size = 'full_image'\n",
    "\n",
    "if strip_size == 'full_image':\n",
    "    y_start = 0\n",
    "    y_end = 64\n",
    "\n",
    "elif strip_size == 'strip':\n",
    "    y_start = 26\n",
    "    y_end = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocess tranformation from prev cell to signal data and pileup data from dataset\n",
    "clean_frames = preprocess(dataset.signal).float().permute(1, 2, 0)[:, y_start:y_end, :].to(device) #pytorch semantics\n",
    "pile_up = preprocess(dataset.pile_up).float().permute(1, 2, 0)[:, y_start:y_end, :].to(device)\n",
    "\n",
    "# Permute changes the order from (C, H, W) (default for PyTorch tensors after ToTensor) to (H, W, C)\n",
    "# This is done to match the common image representation format where the last dimension is the number of channels (e.g., RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader object for the clean_frames dataset\n",
    "\n",
    "# batch_size determines how many samples will be processed together in each iteration during training or evaluation.\n",
    "batch_size = 16  # Adjust as needed, DataLoader will return batches of this many samples at a time\n",
    "\n",
    "dataloader = DataLoader(clean_frames.unsqueeze(1), batch_size=batch_size, shuffle=False)\n",
    "# clean_frames.unsqueeze(1) adds an extra dimension to the tensor, which is necessary to match the expected input shape for the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "#check tensor shape\n",
    "\n",
    "for batch in dataloader:\n",
    "    for tensor in batch:\n",
    "        print(tensor.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luke Johnson\\Anaconda3\\envs\\sd_env\\lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable params:  29654\n"
     ]
    }
   ],
   "source": [
    "from models_stripped_kernels import Model, TrainingConfig, UNetLite_hls #see models.py file\n",
    "\n",
    "modtype = 'UNet_lite' # Change Model type here\n",
    "\n",
    "if modtype == 'UNet2d':\n",
    "    model = Model('UNet', new_dim)\n",
    "    model = model.__getitem__()\n",
    "    config = TrainingConfig(output_dir='retrained_models_UNet2d/temp') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "elif modtype == 'UNet_lite':\n",
    "    model = UNetLite_hls()\n",
    "    config = TrainingConfig(output_dir='trained_models_lite/temp') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "print('Number of learnable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad)) #number of learnable params\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim.AdamW: variant of the Adam optimizer that incorporates weight decay (L2 regularization) to help prevent overfitting\n",
    "# model.parameters(): This passes all the parameters of the model that should be optimized\n",
    "# lr=config.learning_rate: This sets the learning rate for the optimizer, which controls how much to adjust the model parameters at each step of the training process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate) #optimizer updates model parameters to minimise the loss function\n",
    "\n",
    "# get_cosine_schedule_with_warmup: This function creates a learning rate scheduler with a warmup period followed by a cosine decay.\n",
    "# Warmup Period: The learning rate starts at a lower value and gradually increases to the initial learning rate over a specified number of steps (num_warmup_steps).\n",
    "# This helps in stabilizing the training process at the beginning.\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps, # Sets number of warmup steps\n",
    "    num_training_steps=len(dataloader) * config.num_epochs # Calculates total number of training steps based on num of batches per epoch and total num of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_sample, optimizer, train_dataloader, lr_scheduler,noise_scheduler, n_events):\n",
    "\n",
    "#This function defines the main training loop for the model\n",
    "\n",
    "# config: Configuration object containing training settings.\n",
    "# model: The neural network model to be trained.\n",
    "# noise_sample: Noise data used for augmenting the images.\n",
    "# optimizer: Optimizer for updating the model parameters.\n",
    "# train_dataloader: DataLoader providing batches of training data.\n",
    "# lr_scheduler: Learning rate scheduler.\n",
    "# noise_scheduler: Scheduler for adding noise to the images.\n",
    "# n_events: Number of events, possibly related to the dataset size or specific augmentations.\n",
    "###\n",
    "\n",
    "    global_step = torch.tensor(0) # Tensor to keep track of the number of steps taken during training\n",
    "    # Now you train the model\n",
    "    for epoch in range(10):\n",
    "\n",
    "        # Just creating the progress bar\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate over each batch in the training DataLoader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            clean_images = batch\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "            bs = clean_images[0].shape[0] # Batch Size: Extracts the batch size from the first dimension of clean_images.\n",
    "            timesteps = torch.randint(\n",
    "                0, config.num_train_timesteps, (bs,), device=clean_images.device\n",
    "            ).long()\n",
    "\n",
    "            random_seed = np.random.randint(0, n_events)\n",
    "\n",
    "            noisy_images, noise_added = noise_scheduler.add_noise(clean_frame=clean_images, noise_sample=noise_sample, timestep=timesteps, random_seed=random_seed, n_events = n_events)\n",
    "\n",
    "            # Apply saturation value clipping and scaling\n",
    "            noisy_images = torch.clamp(noisy_images, max=saturation_value)\n",
    "\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            noise_added = noise_added.to(device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps)[0] # The model takes the noisy images and timesteps as input and outputs predictions.\n",
    "            loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n",
    "\n",
    "            optimizer.zero_grad() # Clears the old gradients from the last step by setting them to zero. This is necessary because by default, gradients are accumulated in PyTorch.\n",
    "            loss.backward() # Computes the gradient of the loss with respect to the model parameters\n",
    "            optimizer.step() # Updates the model parameters based on the computed gradients\n",
    "            lr_scheduler.step() # Updates the learning rate according to the scheduler's policy\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(config.output_dir, f\"model_epoch_{epoch}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee878e0e8a3045fbb4cbaf32bc6b2945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luke Johnson\\AppData\\Local\\Temp\\ipykernel_14064\\2156798781.py:46: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n",
      "C:\\Users\\Luke Johnson\\AppData\\Local\\Temp\\ipykernel_14064\\2156798781.py:46: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3938bbf570c4b29810973d8e3445ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3621aa71852449c5ad1f21764826e5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72494990753460cb19910d2b29b8200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb7f78bc4c743bdacdcdf5acfd3fd18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc51cde8f00d4560a22faaf508e3f7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ef79ed68e48bbb80fce4c1a377cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c032122bdca49a1a7559578ceabda67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25fc27ac26944f2a665b8549acb8cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551d66a4c95424c910d935e66cc1e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, pile_up, optimizer, dataloader, lr_scheduler, NoiseScheduler('pile-up'), torch.tensor(num_events))\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1) #will port to GPU if availible (can't train on mutli-GPU at Bristol) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
