{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714025946899135\n",
      "CUDA is available!\n",
      "Number of available GPUs: 6\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "from data_processing import Dataset\n",
    "from noise import NoiseScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms  #provides common image transformations for data preprocessing\n",
    "import torch    #used for tensor computations and building neural networks\n",
    "from torch.utils.data import TensorDataset, DataLoader  #TensorDataset wraps tensors. DataLoader creates iterable dataloaders for datasets.\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup # This function creates a learning rate scheduler with a cosine decay and warmup period\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm # Library used to display progress bars for loops, making it easy to track the progress of an iteration\n",
    "from torch.optim import Adam # Popular optimization algorithm used in training neural networks.\n",
    "from pathlib import Path # Object-oriented interface for filesystem paths.\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "print(random.random())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/cephfs/dice/users/ek19824/l1trigger/diffusion/datasets\"       # Old/ wrong path\n",
    "data_dir = \"/cephfs/dice/projects/L1T/diffusion/datasets/\"       # Set to directory where data is stored\n",
    "\n",
    "dataset = Dataset(1000, (120, 72), signal_file=f\"{data_dir}/CaloImages_signal.root\", pile_up_file=f\"{data_dir}/CaloImages_bkg.root\", save=False)\n",
    "# 1000: number of samples in dataset\n",
    "# (120, 72): Shape of each data sample (eg. image with dimensions 120x72)\n",
    "# signal_file: Signal file for the dataset\n",
    "# pile_up_file: This file contains background/ pileup data for the dataset\n",
    "# save=False means the dataset should not be saved to disk after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading file /cephfs/dice/projects/L1T/diffusion/datasets//CaloImages_signal.root\n",
      "loading file /cephfs/dice/projects/L1T/diffusion/datasets//CaloImages_signal.root\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 930.94it/s]\n",
      "INFO:root:loading file /cephfs/dice/projects/L1T/diffusion/datasets//CaloImages_bkg.root\n",
      "loading file /cephfs/dice/projects/L1T/diffusion/datasets//CaloImages_bkg.root\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 921.72it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset() # once this is cached, you don't have to re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim=(64,64) #resize each data sample image into 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:scaling\n",
      "scaling\n",
      "INFO:root:re-sizing\n",
      "re-sizing\n"
     ]
    }
   ],
   "source": [
    "dataset.preprocess(32, new_dim)   # Adjust the saturation energy and image dimensions here\n",
    "# Pixels with an energy greater than the first number (eg.16 or 64 etc) will be clipped and shown as this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from image from dataset from original format into PyTorch tensor\n",
    "preprocess = transforms.Compose( # Chain together multiple image transformations. The transformations are applied sequentially in the order they are specified within the list.\n",
    "        [   \n",
    "            transforms.ToTensor()   # This is a transformation that converts an image (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "                                    # H stands for height, W stands for width, and C stands for the number of channels (e.g., 3 for RGB images).\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOVING DATA TO GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocess tranformation from prev cell to signal data and pileup data from dataset\n",
    "clean_frames = preprocess(dataset.signal).float().permute(1, 2, 0).to(device) #pytorch semantics\n",
    "pile_up = preprocess(dataset.pile_up).float().permute(1, 2, 0).to(device)\n",
    "\n",
    "# Permute changes the order from (C, H, W) (default for PyTorch tensors after ToTensor) to (H, W, C)\n",
    "# This is done to match the common image representation format where the last dimension is the number of channels (e.g., RGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader object for the clean_frames dataset\n",
    "\n",
    "# batch_size determines how many samples will be processed together in each iteration during training or evaluation.\n",
    "batch_size = 16  # Adjust as needed, DataLoader will return batches of this many samples at a time\n",
    "\n",
    "dataloader = DataLoader(clean_frames.unsqueeze(1), batch_size=batch_size, shuffle=False)\n",
    "# clean_frames.unsqueeze(1) adds an extra dimension to the tensor, which is necessary to match the expected input shape for the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "#check tensor shape\n",
    "\n",
    "for batch in dataloader:\n",
    "    for tensor in batch:\n",
    "        print(tensor.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/sa21722/miniconda3/envs/sd_env/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from models import Model, TrainingConfig #see models.py file\n",
    "\n",
    "model = Model('UNet-lite', new_dim)\n",
    "model = model.__getitem__()     #__getitem__() is a class in models.py file. Decides whether to get UNet or UNet-lite\n",
    "\n",
    "config = TrainingConfig(output_dir='trained_models_lite') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #number of learnable params\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim.AdamW: variant of the Adam optimizer that incorporates weight decay (L2 regularization) to help prevent overfitting\n",
    "# model.parameters(): This passes all the parameters of the model that should be optimized\n",
    "# lr=config.learning_rate: This sets the learning rate for the optimizer, which controls how much to adjust the model parameters at each step of the training process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate) #optimizer updates model parameters to minimise the loss function\n",
    "\n",
    "# get_cosine_schedule_with_warmup: This function creates a learning rate scheduler with a warmup period followed by a cosine decay.\n",
    "# Warmup Period: The learning rate starts at a lower value and gradually increases to the initial learning rate over a specified number of steps (num_warmup_steps).\n",
    "# This helps in stabilizing the training process at the beginning.\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps, # Sets number of warmup steps\n",
    "    num_training_steps=len(dataloader) * config.num_epochs # Calculates total number of training steps based on num of batches per epoch and total num of epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_sample, optimizer, train_dataloader, lr_scheduler,noise_scheduler, n_events):\n",
    "\n",
    "#This function defines the main training loop for the model\n",
    "\n",
    "# config: Configuration object containing training settings.\n",
    "# model: The neural network model to be trained.\n",
    "# noise_sample: Noise data used for augmenting the images.\n",
    "# optimizer: Optimizer for updating the model parameters.\n",
    "# train_dataloader: DataLoader providing batches of training data.\n",
    "# lr_scheduler: Learning rate scheduler.\n",
    "# noise_scheduler: Scheduler for adding noise to the images.\n",
    "# n_events: Number of events, possibly related to the dataset size or specific augmentations.\n",
    "###\n",
    "\n",
    "    global_step = torch.tensor(0) # Tensor to keep track of the number of steps taken during training\n",
    "    # Now you train the model\n",
    "    for epoch in range(10):\n",
    "\n",
    "        # Just creating the progress bar\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate over each batch in the training DataLoader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            clean_images = batch\n",
    "            \n",
    "            # Sample noise to add to the images\n",
    "            bs = clean_images[0].shape[0] # Batch Size: Extracts the batch size from the first dimension of clean_images.\n",
    "            timesteps = torch.randint(\n",
    "                0, config.num_train_timesteps, (bs,), device=clean_images.device\n",
    "            ).long()\n",
    "\n",
    "            random_seed = np.random.randint(0, n_events)\n",
    "\n",
    "            noisy_images, noise_added = noise_scheduler.add_noise(clean_frame=clean_images, noise_sample=noise_sample, timestep=timesteps, random_seed=random_seed, n_events = n_events)\n",
    "\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            noise_added = noise_added.to(device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps)[0] # The model takes the noisy images and timesteps as input and outputs predictions.\n",
    "            loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n",
    "\n",
    "            optimizer.zero_grad() # Clears the old gradients from the last step by setting them to zero. This is necessary because by default, gradients are accumulated in PyTorch.\n",
    "            loss.backward() # Computes the gradient of the loss with respect to the model parameters\n",
    "            optimizer.step() # Updates the model parameters based on the computed gradients\n",
    "            lr_scheduler.step() # Updates the learning rate according to the scheduler's policy\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join(config.output_dir, f\"model_epoch_{epoch}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_4018808/2313582712.py:43: UserWarning: Using a target size (torch.Size([64, 64])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n",
      "Epoch 0:  63%|██████▎   | 40/63 [00:00<00:00, 78.73it/s, loss=0.00188, lr=2.16e-5, step=tensor(39)] /tmp/ipykernel_4018808/2313582712.py:43: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(noise_pred, noise_added.float()) # Calculates the mean squared error loss between the predicted noise and the added noise\n",
      "Epoch 0: 100%|██████████| 63/63 [00:00<00:00, 80.40it/s, loss=0.00423, lr=4.76e-5, step=tensor(62)] \n",
      "Epoch 1: 100%|██████████| 63/63 [00:00<00:00, 79.84it/s, loss=0.00367, lr=9.98e-5, step=tensor(125)]\n",
      "Epoch 2: 100%|██████████| 63/63 [00:00<00:00, 84.60it/s, loss=0.00272, lr=5.72e-5, step=tensor(188)] \n",
      "Epoch 3: 100%|██████████| 63/63 [00:00<00:00, 79.79it/s, loss=0.00407, lr=9.31e-7, step=tensor(251)]\n",
      "Epoch 4: 100%|██████████| 63/63 [00:00<00:00, 84.75it/s, loss=0.00288, lr=3.8e-5, step=tensor(314)]  \n",
      "Epoch 5: 100%|██████████| 63/63 [00:00<00:00, 79.74it/s, loss=0.00259, lr=9.79e-5, step=tensor(377)]\n",
      "Epoch 6: 100%|██████████| 63/63 [00:00<00:00, 85.50it/s, loss=0.000102, lr=6.66e-5, step=tensor(440)]\n",
      "Epoch 7: 100%|██████████| 63/63 [00:00<00:00, 80.29it/s, loss=0.000678, lr=3.69e-6, step=tensor(503)]\n",
      "Epoch 8: 100%|██████████| 63/63 [00:00<00:00, 85.37it/s, loss=0.00301, lr=2.89e-5, step=tensor(566)] \n",
      "Epoch 9: 100%|██████████| 63/63 [00:00<00:00, 79.85it/s, loss=0.000832, lr=9.43e-5, step=tensor(629)]\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (config, model, pile_up, optimizer, dataloader, lr_scheduler, NoiseScheduler('pile-up'), torch.tensor(1000))\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1) #will port to GPU if availible (can't train on mutli-GPU at Bristol) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
