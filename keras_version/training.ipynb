{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Use this training notebook to train UNet_lite or UNet2d from: <br> \n",
    "<code><font size=\"3.5\">\n",
    "Models/models_large.py <br>\n",
    "Models/models_reduced_constkernels.py <br>\n",
    "Models/models_reduced_doublingkernels.py <br>\n",
    "</font></code> as it works with their architecture and subclassing API. <br><br>\n",
    "Change as required for your setup:\n",
    "<ul>\n",
    "  <li><code>gpu = gpus[#GPU]</code> Choose #GPU to run training on, can copy and paste notebook and train on different GPUs simultaneously</li>\n",
    "  <li><code>data_dir =</code> Specify path to .root files</li>\n",
    "  <li><code>num_events = </code> How many events to load to .npy and/or train on</li>\n",
    "  <li><code>dataset = Dataset(..., save = True/False,...</code> Use True for 1st run through to create .npy files, False after </li>\n",
    "  <li><code>saturation_value = </code> Choose saturation value</li>\n",
    "  <li><code>output_dir= </code>set where you want your trained model to be saved</li>\n",
    "  <li><code>from Models.</code> FILE </code><code> import ModelConfig, TrainingConfig</code>  Specify FILE of which model architecture from list of .py files above</li>\n",
    "  <li><code>modtype = </code> Choose from either 'UNet_lite' or 'UNet2d'</li>\n",
    "  <li><code>strip_size = </code> Choose from either 'strip' or 'full_image'</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 19:37:37.572379: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:numexpr.utils:Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 6\n",
      "Current GPU: PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm # Library used to display progress bars for loops, making it easy to track the progress of an iteration\n",
    "from data_processing import Dataset\n",
    "from noise import NoiseScheduler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 22\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set the device to custom GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpu = gpus[4] # Specify which gpu to use here. Can run multiple scripts on different GPUs\n",
    "if gpu:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        print(\"CUDA is available!\")\n",
    "        print(\"Number of available GPUs:\", len(gpus))\n",
    "        print(\"Current GPU:\", gpu)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to directory where data is stored\n",
    "work_home = False\n",
    "data_dir = \"Datasets\" if work_home else \"/cephfs/dice/projects/L1T/diffusion/datasets/\"\n",
    "\n",
    "num_events = 10000 # Adjust number of events to train model here\n",
    "start_idx = 0\n",
    "end_idx = num_events\n",
    "dataset = Dataset(num_events, (120, 72), signal_file=f\"{data_dir}/CaloImages_signal.root\", pile_up_file=f\"{data_dir}/CaloImages_bkg.root\", save=False, start_idx=start_idx, end_idx=end_idx) # Can set to 10000\n",
    "# 1000: number of samples in dataset\n",
    "# (120, 72): Shape of each data sample (eg. image with dimensions 120x72)\n",
    "# signal_file: Signal file for the dataset\n",
    "# pile_up_file: This file contains background/ pileup data for the dataset\n",
    "# save=False means the dataset should not be saved to disk after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading .npy files from /storage/sa21722/stablediffusion/keras_version/signal.npy and /storage/sa21722/stablediffusion/keras_version/pile_up.npy\n"
     ]
    }
   ],
   "source": [
    "dataset() # once this is cached, you don't have to re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Re-sizing tensors...\n"
     ]
    }
   ],
   "source": [
    "saturation_value = 512 # Change saturation energy here\n",
    "dataset.preprocess((64,64))\n",
    "# Pixels with an energy greater than the first number (eg.16 or 64 etc) will be clipped and shown as this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract horizontal strip from y=26 to y=38 (12 pixels tall)\n",
    "# Change how much of image to train model on here\n",
    "strip_size = 'full_image'\n",
    "\n",
    "if strip_size == 'full_image':\n",
    "    y_start = 0\n",
    "    y_end = 64\n",
    "\n",
    "elif strip_size == 'strip':\n",
    "    y_start = 26\n",
    "    y_end = 38\n",
    "\n",
    "new_dim=(y_end-y_start,64) #resize each data sample image into 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 19:37:43.383006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 19:37:43.635623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13664 MB memory:  -> device: 4, name: Tesla T4, pci bus id: 0000:c1:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64, 64, 1)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Convert data to TensorFlow tensors\n",
    "clean_frames = tf.convert_to_tensor(dataset.signal, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "pile_up = tf.convert_to_tensor(dataset.pile_up, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "\n",
    "# Normalize data\n",
    "clean_frames = tf.clip_by_value(clean_frames, 0, saturation_value)\n",
    "pile_up = tf.clip_by_value(pile_up, 0, saturation_value)\n",
    "\n",
    "# Reshape data\n",
    "clean_frames = tf.expand_dims(clean_frames, axis=-1)\n",
    "pile_up = tf.expand_dims(pile_up, axis=-1)\n",
    "\n",
    "print(clean_frames.shape)\n",
    "print(clean_frames.dtype)\n",
    "# Permute changes the order to (B, H, W, C)\n",
    "# This is done to match the common image representation format where the last dimension is the number of channels (e.g., RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader object for the clean_frames dataset\n",
    "# batch_size determines how many samples will be processed together in each iteration during training or evaluation.\n",
    "batch_size = 16\n",
    "#dataloader = tf.data.Dataset.from_tensor_slices(clean_frames).batch(batch_size)\n",
    "dataloader = (\n",
    "    tf.data.Dataset.from_tensor_slices(clean_frames)\n",
    "    .shuffle(buffer_size=len(clean_frames))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 19:37:44.759159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8902\n",
      "2024-08-10 19:37:44.834044: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNetLite_hls\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " positional_encoding (Positi  multiple                 0         \n",
      " onalEncoding)                                                   \n",
      "                                                                 \n",
      " emb1 (Dense)                multiple                  5         \n",
      "                                                                 \n",
      " convd1_1 (Conv2D)           multiple                  36        \n",
      "                                                                 \n",
      " normd1_1 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu1 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " convd1_2 (Conv2D)           multiple                  144       \n",
      "                                                                 \n",
      " normd1_2 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu2 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " pool3 (Conv2D)              multiple                  144       \n",
      "                                                                 \n",
      " normb1_1 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu3 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " emb4 (Dense)                multiple                  20        \n",
      "                                                                 \n",
      " convb1_1 (Conv2D)           multiple                  288       \n",
      "                                                                 \n",
      " normb1_2 (LayerNormalizatio  multiple                 16        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu4 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " convb1_2 (Conv2D)           multiple                  576       \n",
      "                                                                 \n",
      " normb1_3 (LayerNormalizatio  multiple                 16        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu5 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " up1 (Conv2DTranspose)       multiple                  292       \n",
      "                                                                 \n",
      " normu1_1 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu6 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " emb5 (Dense)                multiple                  40        \n",
      "                                                                 \n",
      " convu1_1 (Conv2D)           multiple                  288       \n",
      "                                                                 \n",
      " normu1_2 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu7 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " convu1_2 (Conv2D)           multiple                  144       \n",
      "                                                                 \n",
      " normu1_3 (LayerNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " relu8 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " out (Conv2D)                multiple                  5         \n",
      "                                                                 \n",
      " relu9 (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,062\n",
      "Trainable params: 2,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from Models.models_reduced_doublingkernels import Model, TrainingConfig\n",
    "\n",
    "modtype = 'UNet_lite' # Change Model type here\n",
    "model = Model(modtype, new_dim)\n",
    "model = model.__getitem__()\n",
    "\n",
    "if modtype == 'UNet2d':\n",
    "    config = TrainingConfig(output_dir='retrained_models_UNet2d/temp') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "elif modtype == 'UNet_lite':\n",
    "    config = TrainingConfig(output_dir='trained_models_lite/temp') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "\n",
    "# Verify model initialization\n",
    "dummy_input = tf.zeros((1, 64, 64, 1), dtype=tf.float32)\n",
    "dummy_time = tf.zeros((), dtype=tf.int32)\n",
    "model([dummy_input, dummy_time])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=config.learning_rate,\n",
    "    decay_steps=len(dataloader) * config.num_epochs,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(model, optimizer, noisy_images, noise_added, timestep, loss_fn, saturation_value, modtype):\n",
    "    # Apply saturation value clipping and scaling\n",
    "    noisy_images = tf.clip_by_value(noisy_images, 0, saturation_value)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict the noise residual\n",
    "        if modtype == 'UNet2d':\n",
    "            noise_pred = model([noisy_images, timestep], training=True)[0]\n",
    "        elif modtype == 'UNet_lite':\n",
    "            noise_pred = model([noisy_images, timestep], training=True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(noise_added, noise_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_loop(config, model, noise_sample, optimizer, train_dataloader, noise_scheduler, n_events, loss_fn, saturation_value, modtype):\n",
    "\n",
    "    global_step = 0  # Counter to keep track of the number of steps taken during training\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate over each batch in the training DataLoader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            timestep = tf.random.uniform((), minval=0, maxval=config.num_train_timesteps, dtype=tf.int32)\n",
    "            random_seed = np.random.randint(0, n_events)\n",
    "            \n",
    "            noisy_images, noise_added = noise_scheduler.add_noise(\n",
    "                clean_frame=batch, \n",
    "                noise_sample=noise_sample, \n",
    "                timestep=timestep, \n",
    "                random_seed=random_seed, \n",
    "                n_events=n_events\n",
    "            )\n",
    "            \n",
    "            # Perform the training step\n",
    "            loss = train_step(model, optimizer, noisy_images, noise_added, timestep, loss_fn, saturation_value, modtype)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.numpy(), \"lr\": optimizer.learning_rate.numpy(), \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model.save_weights(os.path.join(config.output_dir, f\"model_epoch_{epoch}.h5\"))  # USE MODEL.SAVE() TO SAVE MODEL ARCHITECTURE ALSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a1b4d3a62046efb4c06c43c68876ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 19:37:47.827044: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fa8e1ae14f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-10 19:37:47.827105: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-08-10 19:37:47.831519: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-10 19:37:47.891970: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-08-10 19:37:47.907810: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513ce4c912c0460aa9e0e83f47ecd681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ee7339508f4958a61b53d06a4e9e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540602aad99d4b9aa0435a9c777e4029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd57a6a13cf943828b43a64348e0ce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b468df87974f9db82dbde44f9a5a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59aa6cac1d384d09b51258b2a1012067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b7a64ddca642098a4948443e3dc759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b07bfdb1ccc4821aa06e487d6b5dbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981a2ddccc71427585fdbb7f947c34f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the training loop\n",
    "train_loop(config, model, pile_up, optimizer, dataloader, NoiseScheduler('pile-up'), num_events, loss_fn, saturation_value, modtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
