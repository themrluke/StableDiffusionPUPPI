{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 11:19:17.159879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 6\n",
      "Current GPU: PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm # Library used to display progress bars for loops, making it easy to track the progress of an iteration\n",
    "from data_processing import Dataset\n",
    "from noise import NoiseScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "seed = 22\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set the device to custom GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpu = gpus[5] # Specify which gpu to use here. Can run multiple scripts on different GPUs\n",
    "if gpu:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        print(\"CUDA is available!\")\n",
    "        print(\"Number of available GPUs:\", len(gpus))\n",
    "        print(\"Current GPU:\", gpu)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to directory where data is stored\n",
    "work_home = False\n",
    "data_dir = \"Datasets\" if work_home else \"/cephfs/dice/projects/L1T/diffusion/datasets/\"\n",
    "\n",
    "num_events = 100000 # Adjust number of events to train model here\n",
    "start_idx = 0\n",
    "end_idx = num_events\n",
    "dataset = Dataset(num_events, (120, 72), signal_file=f\"{data_dir}/CaloImages_signal.root\", pile_up_file=f\"{data_dir}/CaloImages_bkg.root\", save=False, start_idx=start_idx, end_idx=end_idx) # Can set to 10000\n",
    "# 1000: number of samples in dataset\n",
    "# (120, 72): Shape of each data sample (eg. image with dimensions 120x72)\n",
    "# signal_file: Signal file for the dataset\n",
    "# pile_up_file: This file contains background/ pileup data for the dataset\n",
    "# save=False means the dataset should not be saved to disk after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset() # once this is cached, you don't have to re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim=(64,64) #resize each data sample image into 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:re-sizing\n",
      "re-sizing\n"
     ]
    }
   ],
   "source": [
    "saturation_value = 512 # Change saturation energy here\n",
    "dataset.preprocess(new_dim)\n",
    "# Pixels with an energy greater than the first number (eg.16 or 64 etc) will be clipped and shown as this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract horizontal strip from y=26 to y=38 (12 pixels tall)\n",
    "# Change how much of image to train model on here\n",
    "strip_size = 'strip'\n",
    "\n",
    "if strip_size == 'full_image':\n",
    "    y_start = 0\n",
    "    y_end = 64\n",
    "\n",
    "elif strip_size == 'strip':\n",
    "    y_start = 26\n",
    "    y_end = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 11:19:26.663542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 11:19:26.931677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 5, name: Tesla T4, pci bus id: 0000:e2:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 12, 64, 1)\n",
      "<dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Convert data to TensorFlow tensors\n",
    "clean_frames = tf.convert_to_tensor(dataset.signal, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "pile_up = tf.convert_to_tensor(dataset.pile_up, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "\n",
    "# Normalize data\n",
    "clean_frames = tf.clip_by_value(clean_frames, 0, saturation_value)\n",
    "pile_up = tf.clip_by_value(pile_up, 0, saturation_value)\n",
    "\n",
    "# Reshape data\n",
    "clean_frames = tf.expand_dims(clean_frames, axis=-1)\n",
    "pile_up = tf.expand_dims(pile_up, axis=-1)\n",
    "\n",
    "print(clean_frames.shape)\n",
    "print(clean_frames.dtype)\n",
    "# Permute changes the order to (B, H, W, C)\n",
    "# This is done to match the common image representation format where the last dimension is the number of channels (e.g., RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader object for the clean_frames dataset\n",
    "# batch_size determines how many samples will be processed together in each iteration during training or evaluation.\n",
    "batch_size = 16\n",
    "#dataloader = tf.data.Dataset.from_tensor_slices(clean_frames).batch(batch_size)\n",
    "dataloader = (\n",
    "    tf.data.Dataset.from_tensor_slices(clean_frames)\n",
    "    .shuffle(buffer_size=len(clean_frames))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 11:19:30.412831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8902\n",
      "2024-07-19 11:19:30.488049: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable params:  4950\n",
      "Model: \"u_net_lite_hls\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " positional_encoding (Positi  multiple                 0         \n",
      " onalEncoding)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  5         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             multiple                  72        \n",
      "                                                                 \n",
      " layer_normalization (LayerN  multiple                 16        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  576       \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  multiple                 16        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           multiple                  576       \n",
      "                                                                 \n",
      " layer_normalization_2 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_4 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_5 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_6 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_7 (Laye  multiple                 0 (unused)\n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_8 (Laye  multiple                 16        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  40        \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           multiple                  576       \n",
      "                                                                 \n",
      " layer_normalization_9 (Laye  multiple                 16        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " attention (Attention)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          multiple                  576       \n",
      "                                                                 \n",
      " layer_normalization_11 (Lay  multiple                 16        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  multiple                 584       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " layer_normalization_12 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_13 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_14 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  multiple                 0 (unused)\n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " layer_normalization_15 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_16 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          multiple                  0 (unused)\n",
      "                                                                 \n",
      " layer_normalization_17 (Lay  multiple                 0 (unused)\n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  multiple                 0 (unused)\n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " layer_normalization_18 (Lay  multiple                 16        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  80        \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          multiple                  1152      \n",
      "                                                                 \n",
      " layer_normalization_19 (Lay  multiple                 16        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          multiple                  576       \n",
      "                                                                 \n",
      " layer_normalization_20 (Lay  multiple                 16        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          multiple                  9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,950\n",
      "Trainable params: 4,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from models_reduced_constkernels import Model, TrainingConfig, UNetLite_hls #see models.py file\n",
    "\n",
    "modtype = 'UNet_lite' # Change Model type here\n",
    "\n",
    "if modtype == 'UNet2d':\n",
    "    model = Model('UNet', new_dim)\n",
    "    model = model.__getitem__()\n",
    "    config = TrainingConfig(output_dir='retrained_models_UNet2d/temp') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "elif modtype == 'UNet_lite':\n",
    "    model = UNetLite_hls()\n",
    "    config = TrainingConfig(output_dir='trained_models_lite/reduced_constkernels_strip') # Holds parameters used for training the model eg. learning rate, image size, number of epochs ....\n",
    "\n",
    "\n",
    "# In your model, ensure timestep input is int32\n",
    "dummy_input = tf.zeros((1, new_dim[0], new_dim[1], 1), dtype=tf.float32)\n",
    "dummy_time = tf.zeros((), dtype=tf.int32)\n",
    "model((dummy_input, dummy_time))\n",
    "print('Number of learnable params: ', model.count_params())\n",
    "print(model.summary()) # PRINT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=config.learning_rate,\n",
    "    decay_steps=len(dataloader) * config.num_epochs,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(model, optimizer, noisy_images, noise_added, timestep, loss_fn, saturation_value, modtype):\n",
    "    # Apply saturation value clipping and scaling\n",
    "    noisy_images = tf.clip_by_value(noisy_images, 0, saturation_value)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict the noise residual\n",
    "        if modtype == 'UNet2d':\n",
    "            noise_pred = model([noisy_images, timestep], training=True)[0]\n",
    "        elif modtype == 'UNet_lite':\n",
    "            noise_pred = model([noisy_images, timestep], training=True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(noise_added, noise_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_loop(config, model, noise_sample, optimizer, train_dataloader, noise_scheduler, n_events, loss_fn, saturation_value, modtype):\n",
    "\n",
    "    global_step = 0  # Counter to keep track of the number of steps taken during training\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate over each batch in the training DataLoader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch.numpy()\n",
    "            bs = clean_images.shape[0]  # Batch size\n",
    "            timestep = tf.random.uniform((), minval=0, maxval=config.num_train_timesteps, dtype=tf.int32)\n",
    "            \n",
    "            random_seed = np.random.randint(0, n_events)\n",
    "            \n",
    "            noisy_images, noise_added = noise_scheduler.add_noise(\n",
    "                clean_frame=clean_images, \n",
    "                noise_sample=noise_sample, \n",
    "                timestep=timestep, \n",
    "                random_seed=random_seed, \n",
    "                n_events=n_events\n",
    "            )\n",
    "            \n",
    "            # Perform the training step\n",
    "            loss = train_step(model, optimizer, noisy_images, noise_added, timestep, loss_fn, saturation_value, modtype)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.numpy(), \"lr\": optimizer.learning_rate.numpy(), \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model.save_weights(os.path.join(config.output_dir, f\"model_epoch_{epoch}.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c9d63a0a764b8eb0ba045fa1b943ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 11:19:33.290795: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0xd93ae60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-19 11:19:33.290843: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-07-19 11:19:33.295280: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-19 11:19:33.354673: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-19 11:19:33.370626: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c09260c0a443f198a65ce2391ab22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4117465058640d6a0aed2f5b676ed11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09f0de4d3d64f899d6e1cb91b40da52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1249712c3c246feabc59be09f752ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abb7823521e4a2d9802afaa1a2d35e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bcf183ac8a4222be4c73ec686015c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13926b75ea54512ae28aad69484ff4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13ddbd69fe4a6a9224f83d1e539b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4522b34e964655aa44eb80f6503faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the training loop\n",
    "train_loop(config, model, pile_up, optimizer, dataloader, NoiseScheduler('pile-up'), num_events, loss_fn, saturation_value, modtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
