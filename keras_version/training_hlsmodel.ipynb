{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Use this training notebook to train UNet_lite from: <br> \n",
    "<code><font size=\"3.5\">\n",
    "Models/model_for_hls.py <br>\n",
    "</font></code> as it works with its different architecture and functional API. <br><br>\n",
    "Change as required for your setup:\n",
    "<ul>\n",
    "  <li><code>gpu = gpus[#GPU]</code> Choose #GPU to run training on, can copy and paste notebook and train on different GPUs simultaneously</li>\n",
    "  <li><code>num_events = </code> How many events to load to .npy and/or train on</li>\n",
    "  <li><code>dataset = Dataset(..., save = True/False,...</code> Use True for 1st run through to create .npy files, False after </li>\n",
    "  <li><code>gpu = gpus[#GPU]</code> Choose #GPU to run training on, can copy and paste notebook and train on different GPUs simultaneously</li>\n",
    "  <li><code>output_dir= </code>set where you want your trained model to be saved</li>\n",
    "  <li><code>modtype = </code> Choose from either 'UNet_lite' or 'UNet2d'</li>\n",
    "  <li><code>strip_size = </code> Choose from either 'strip' or 'full_image'</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:38:14.157501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 16:38:14.215746: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of available GPUs: 1\n",
      "Current GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:38:16.611111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:16.630037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:16.630066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm # Library used to display progress bars for loops, making it easy to track the progress of an iteration\n",
    "from data_processing import Dataset\n",
    "from noise import NoiseScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 22\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Set the device to custom GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpu = gpus[0] # Specify which gpu to use here. Can run multiple scripts on different GPUs\n",
    "if gpu:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "        print(\"CUDA is available!\")\n",
    "        print(\"Number of available GPUs:\", len(gpus))\n",
    "        print(\"Current GPU:\", gpu)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to directory where data is stored\n",
    "work_home = True\n",
    "data_dir = \"Datasets\" if work_home else \"/cephfs/dice/projects/L1T/diffusion/datasets/\"\n",
    "\n",
    "num_events = 256 # Adjust number of events to train model here\n",
    "start_idx = 0\n",
    "end_idx = num_events\n",
    "dataset = Dataset(num_events, (120, 72), signal_file=f\"{data_dir}/CaloImages_signal.root\", pile_up_file=f\"{data_dir}/CaloImages_bkg.root\", save=False, start_idx=start_idx, end_idx=end_idx) # Can set to 10000\n",
    "# 1000: number of samples in dataset\n",
    "# (120, 72): Shape of each data sample (eg. image with dimensions 120x72)\n",
    "# signal_file: Signal file for the dataset\n",
    "# pile_up_file: This file contains background/ pileup data for the dataset\n",
    "# save=False means the dataset should not be saved to disk after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading .npy files from /home/themrluke/projects/stablediffusion/keras_version/signal.npy and /home/themrluke/projects/stablediffusion/keras_version/pile_up.npy\n"
     ]
    }
   ],
   "source": [
    "dataset() # once this is cached, you don't have to re-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dim=(64,64) #resize each data sample image into 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Re-sizing tensors...\n"
     ]
    }
   ],
   "source": [
    "saturation_value = 512 # Change saturation energy here\n",
    "dataset.preprocess(new_dim)\n",
    "# Pixels with an energy greater than the first number (eg.16 or 64 etc) will be clipped and shown as this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract horizontal strip from y=26 to y=38 (12 pixels tall)\n",
    "# Change how much of image to train model on here\n",
    "strip_size = 'full_image'\n",
    "\n",
    "if strip_size == 'full_image':\n",
    "    y_start = 0\n",
    "    y_end = 64\n",
    "\n",
    "elif strip_size == 'strip':\n",
    "    y_start = 26\n",
    "    y_end = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 64, 1)\n",
      "<dtype: 'float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:38:17.671989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 16:38:17.673663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.673691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.673703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.736205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.736238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.736243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-29 16:38:17.736259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-29 16:38:17.736278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9502 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Convert data to TensorFlow tensors\n",
    "clean_frames = tf.convert_to_tensor(dataset.signal, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "pile_up = tf.convert_to_tensor(dataset.pile_up, dtype=tf.float32)[:, y_start:y_end, :]\n",
    "\n",
    "# Normalize data\n",
    "clean_frames = tf.clip_by_value(clean_frames, 0, saturation_value)\n",
    "pile_up = tf.clip_by_value(pile_up, 0, saturation_value)\n",
    "\n",
    "# Reshape data\n",
    "clean_frames = tf.expand_dims(clean_frames, axis=-1)\n",
    "pile_up = tf.expand_dims(pile_up, axis=-1)\n",
    "\n",
    "print(clean_frames.shape)\n",
    "print(clean_frames.dtype)\n",
    "# Permute changes the order to (B, H, W, C)\n",
    "# This is done to match the common image representation format where the last dimension is the number of channels (e.g., RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataLoader object for the clean_frames dataset\n",
    "# batch_size determines how many samples will be processed together in each iteration during training or evaluation.\n",
    "batch_size = 16\n",
    "#dataloader = tf.data.Dataset.from_tensor_slices(clean_frames).batch(batch_size)\n",
    "dataloader = (\n",
    "    tf.data.Dataset.from_tensor_slices(clean_frames)\n",
    "    .shuffle(buffer_size=len(clean_frames))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNetLite_hls\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " pos_encoding_main (InputLayer)  [(None, 64, 64, 4)]  0          []                               \n",
      "                                                                                                  \n",
      " input_images (InputLayer)      [(None, 64, 64, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " emb1 (Dense)                   (None, 64, 64, 1)    5           ['pos_encoding_main[0][0]']      \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 64, 1)    0           ['input_images[0][0]',           \n",
      "                                                                  'emb1[0][0]']                   \n",
      "                                                                                                  \n",
      " convd1_1 (Conv2D)              (None, 64, 64, 4)    36          ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " normd1_1 (BatchNormalization)  (None, 64, 64, 4)    16          ['convd1_1[0][0]']               \n",
      "                                                                                                  \n",
      " relu_1 (ReLU)                  (None, 64, 64, 4)    0           ['normd1_1[0][0]']               \n",
      "                                                                                                  \n",
      " convd1_2 (Conv2D)              (None, 64, 64, 4)    144         ['relu_1[0][0]']                 \n",
      "                                                                                                  \n",
      " normd1_2 (BatchNormalization)  (None, 64, 64, 4)    16          ['convd1_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_2 (ReLU)                  (None, 64, 64, 4)    0           ['normd1_2[0][0]']               \n",
      "                                                                                                  \n",
      " pool3 (Conv2D)                 (None, 32, 32, 4)    144         ['relu_2[0][0]']                 \n",
      "                                                                                                  \n",
      " normb1_1 (BatchNormalization)  (None, 32, 32, 4)    16          ['pool3[0][0]']                  \n",
      "                                                                                                  \n",
      " pos_encoding_bottleneck (Input  [(None, 32, 32, 4)]  0          []                               \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " relu_3 (ReLU)                  (None, 32, 32, 4)    0           ['normb1_1[0][0]']               \n",
      "                                                                                                  \n",
      " emb4 (Dense)                   (None, 32, 32, 4)    20          ['pos_encoding_bottleneck[0][0]']\n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 4)    0           ['relu_3[0][0]',                 \n",
      "                                                                  'emb4[0][0]']                   \n",
      "                                                                                                  \n",
      " convb1_1 (Conv2D)              (None, 32, 32, 8)    288         ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " normb1_2 (BatchNormalization)  (None, 32, 32, 8)    32          ['convb1_1[0][0]']               \n",
      "                                                                                                  \n",
      " relu_4 (ReLU)                  (None, 32, 32, 8)    0           ['normb1_2[0][0]']               \n",
      "                                                                                                  \n",
      " convb1_2 (Conv2D)              (None, 32, 32, 8)    576         ['relu_4[0][0]']                 \n",
      "                                                                                                  \n",
      " normb1_3 (BatchNormalization)  (None, 32, 32, 8)    32          ['convb1_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_5 (ReLU)                  (None, 32, 32, 8)    0           ['normb1_3[0][0]']               \n",
      "                                                                                                  \n",
      " up1 (UpSampling2D)             (None, 64, 64, 8)    0           ['relu_5[0][0]']                 \n",
      "                                                                                                  \n",
      " convu1_1 (Conv2D)              (None, 64, 64, 4)    292         ['up1[0][0]']                    \n",
      "                                                                                                  \n",
      " normu1_1 (BatchNormalization)  (None, 64, 64, 4)    16          ['convu1_1[0][0]']               \n",
      "                                                                                                  \n",
      " relu_6 (ReLU)                  (None, 64, 64, 4)    0           ['normu1_1[0][0]']               \n",
      "                                                                                                  \n",
      " emb5 (Dense)                   (None, 64, 64, 4)    20          ['pos_encoding_main[0][0]']      \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64, 4)    0           ['relu_6[0][0]',                 \n",
      "                                                                  'emb5[0][0]']                   \n",
      "                                                                                                  \n",
      " convu1_2 (Conv2D)              (None, 64, 64, 4)    144         ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " normu1_2 (BatchNormalization)  (None, 64, 64, 4)    16          ['convu1_2[0][0]']               \n",
      "                                                                                                  \n",
      " relu_7 (ReLU)                  (None, 64, 64, 4)    0           ['normu1_2[0][0]']               \n",
      "                                                                                                  \n",
      " convu1_3 (Conv2D)              (None, 64, 64, 4)    144         ['relu_7[0][0]']                 \n",
      "                                                                                                  \n",
      " normu1_3 (BatchNormalization)  (None, 64, 64, 4)    16          ['convu1_3[0][0]']               \n",
      "                                                                                                  \n",
      " relu_8 (ReLU)                  (None, 64, 64, 4)    0           ['normu1_3[0][0]']               \n",
      "                                                                                                  \n",
      " out (Conv2D)                   (None, 64, 64, 1)    5           ['relu_8[0][0]']                 \n",
      "                                                                                                  \n",
      " input_time (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " relu_9 (ReLU)                  (None, 64, 64, 1)    0           ['out[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,978\n",
      "Trainable params: 1,898\n",
      "Non-trainable params: 80\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import the UNet model\n",
    "from Models.model_for_hls import Model, TrainingConfig\n",
    "\n",
    "modtype = 'UNet_lite'  # Change Model type here\n",
    "model = Model(modtype, new_dim)\n",
    "\n",
    "config = TrainingConfig(output_dir='trained_models_lite/temp')\n",
    "model = model.__getitem__(batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=config.learning_rate,\n",
    "    decay_steps=len(dataloader) * config.num_epochs,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Compile the model (this step is to set up the internal state correctly)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.model_for_hls import positional_encoding\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(model, optimizer, noisy_images, noise_added, timestep, pos_encoding, pos_encoding_bottleneck, loss_fn, saturation_value, modtype):\n",
    "    # Apply saturation value clipping and scaling\n",
    "    noisy_images = tf.clip_by_value(noisy_images, 0, saturation_value)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        # Reshape timestep to the correct shape\n",
    "        timestep = tf.reshape(timestep, [-1, 1])\n",
    "        \n",
    "        # Predict the noise residual\n",
    "        if modtype == 'UNet2d':\n",
    "            noise_pred = model([noisy_images, timestep, pos_encoding, pos_encoding_bottleneck], training=True)[0]\n",
    "        elif modtype == 'UNet_lite':\n",
    "            noise_pred = model([noisy_images, timestep, pos_encoding, pos_encoding_bottleneck], training=True)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(noise_added, noise_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_loop(config, model, noise_sample, optimizer, train_dataloader, noise_scheduler, n_events, loss_fn, saturation_value, modtype):\n",
    "\n",
    "    global_step = 0  # Counter to keep track of the number of steps taken during training\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # Iterate over each batch in the training DataLoader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_size = batch.shape[0]  # Determine batch size dynamically\n",
    "            timestep = tf.random.uniform((), minval=0, maxval=config.num_train_timesteps, dtype=tf.int32)\n",
    "            \n",
    "            random_seed = np.random.randint(0, n_events)\n",
    "            \n",
    "            noisy_images, noise_added = noise_scheduler.add_noise(\n",
    "                clean_frame=batch, \n",
    "                noise_sample=noise_sample, \n",
    "                timestep=timestep, \n",
    "                random_seed=random_seed, \n",
    "                n_events=n_events\n",
    "            )\n",
    "            \n",
    "            # Compute positional encodings\n",
    "            pos_encoding = positional_encoding(timestep, batch_size, (64, 64), 4, 5000)\n",
    "            pos_encoding_bottleneck = positional_encoding(timestep, batch_size, (32, 32), 4, 5000)\n",
    "            \n",
    "            # Perform the training step\n",
    "            loss = train_step(model, optimizer, noisy_images, noise_added, timestep, pos_encoding, pos_encoding_bottleneck, loss_fn, saturation_value, modtype)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.numpy(), \"lr\": optimizer.learning_rate.numpy(), \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        model.save(os.path.join(config.output_dir, f\"model_epoch_{epoch}.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae329a108f74bd6a0250c444e2315b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 16:38:18.936570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-07-29 16:38:18.946174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8902\n",
      "2024-07-29 16:38:19.002944: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2024-07-29 16:38:19.272483: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fd9280d1650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 16:38:19.272510: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2024-07-29 16:38:19.275168: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 16:38:19.321088: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2024-07-29 16:38:19.329869: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00447397c0d4f4e8d0e7b48a066cb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7becf7c4cc412ebb3dc20f019cba08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d925422715421e9e44ab4638e8b55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8993f6c150be431584e500712cb5bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce67c2b5de74c58b4103b421b2b0bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ecba67999b4a7daf72d2def6baa976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6d2a71582b4b96bcdefcd1d19fd33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47225fd80c684553ac629d4cef079a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217c0c1b544c4dbe9536782e95b74a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the training loop\n",
    "train_loop(config, model, pile_up, optimizer, dataloader, NoiseScheduler('pile-up'), num_events, loss_fn, saturation_value, modtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
