2024-08-12 15:06:52.301969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-12 15:07:18.787600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-12 15:07:21.541987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 531 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:01:00.0, compute capability: 7.5
2024-08-12 15:07:21.552674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 271 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:21:00.0, compute capability: 7.5
2024-08-12 15:07:21.555567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 211 MB memory:  -> device: 2, name: Tesla T4, pci bus id: 0000:41:00.0, compute capability: 7.5
2024-08-12 15:07:21.561553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13764 MB memory:  -> device: 3, name: Tesla T4, pci bus id: 0000:a1:00.0, compute capability: 7.5
2024-08-12 15:07:21.564658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 13764 MB memory:  -> device: 4, name: Tesla T4, pci bus id: 0000:c1:00.0, compute capability: 7.5
2024-08-12 15:07:21.566902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 13764 MB memory:  -> device: 5, name: Tesla T4, pci bus id: 0000:e2:00.0, compute capability: 7.5
Loading model
Done
Model: "UNetLite_hls"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 pos_enc_main (InputLayer)      [(None, 4, 64, 4)]   0           []                               
                                                                                                  
 input_images (InputLayer)      [(None, 4, 64, 1)]   0           []                               
                                                                                                  
 emb1 (Dense)                   (None, 4, 64, 1)     5           ['pos_enc_main[0][0]']           
                                                                                                  
 add (Add)                      (None, 4, 64, 1)     0           ['input_images[0][0]',           
                                                                  'emb1[0][0]']                   
                                                                                                  
 convd1_1 (Conv2D)              (None, 4, 64, 4)     36          ['add[0][0]']                    
                                                                                                  
 normd1_1 (BatchNormalization)  (None, 4, 64, 4)     16          ['convd1_1[0][0]']               
                                                                                                  
 relu_1 (ReLU)                  (None, 4, 64, 4)     0           ['normd1_1[0][0]']               
                                                                                                  
 convd1_2 (Conv2D)              (None, 4, 64, 4)     144         ['relu_1[0][0]']                 
                                                                                                  
 normd1_2 (BatchNormalization)  (None, 4, 64, 4)     16          ['convd1_2[0][0]']               
                                                                                                  
 relu_2 (ReLU)                  (None, 4, 64, 4)     0           ['normd1_2[0][0]']               
                                                                                                  
 pool3 (Conv2D)                 (None, 2, 32, 4)     144         ['relu_2[0][0]']                 
                                                                                                  
 normb1_1 (BatchNormalization)  (None, 2, 32, 4)     16          ['pool3[0][0]']                  
                                                                                                  
 pos_enc_bottleneck (InputLayer  [(None, 2, 32, 4)]  0           []                               
 )                                                                                                
                                                                                                  
 relu_3 (ReLU)                  (None, 2, 32, 4)     0           ['normb1_1[0][0]']               
                                                                                                  
 emb4 (Dense)                   (None, 2, 32, 4)     20          ['pos_enc_bottleneck[0][0]']     
                                                                                                  
 add_1 (Add)                    (None, 2, 32, 4)     0           ['relu_3[0][0]',                 
                                                                  'emb4[0][0]']                   
                                                                                                  
 convb1_1 (Conv2D)              (None, 2, 32, 8)     288         ['add_1[0][0]']                  
                                                                                                  
 normb1_2 (BatchNormalization)  (None, 2, 32, 8)     32          ['convb1_1[0][0]']               
                                                                                                  
 relu_4 (ReLU)                  (None, 2, 32, 8)     0           ['normb1_2[0][0]']               
                                                                                                  
 convb1_2 (Conv2D)              (None, 2, 32, 8)     576         ['relu_4[0][0]']                 
                                                                                                  
 normb1_3 (BatchNormalization)  (None, 2, 32, 8)     32          ['convb1_2[0][0]']               
                                                                                                  
 relu_5 (ReLU)                  (None, 2, 32, 8)     0           ['normb1_3[0][0]']               
                                                                                                  
 up1 (UpSampling2D)             (None, 4, 64, 8)     0           ['relu_5[0][0]']                 
                                                                                                  
 convu1_1 (Conv2D)              (None, 4, 64, 4)     292         ['up1[0][0]']                    
                                                                                                  
 normu1_1 (BatchNormalization)  (None, 4, 64, 4)     16          ['convu1_1[0][0]']               
                                                                                                  
 relu_6 (ReLU)                  (None, 4, 64, 4)     0           ['normu1_1[0][0]']               
                                                                                                  
 emb5 (Dense)                   (None, 4, 64, 4)     20          ['pos_enc_main[0][0]']           
                                                                                                  
 add_2 (Add)                    (None, 4, 64, 4)     0           ['relu_6[0][0]',                 
                                                                  'emb5[0][0]']                   
                                                                                                  
 convu1_2 (Conv2D)              (None, 4, 64, 4)     144         ['add_2[0][0]']                  
                                                                                                  
 normu1_2 (BatchNormalization)  (None, 4, 64, 4)     16          ['convu1_2[0][0]']               
                                                                                                  
 relu_7 (ReLU)                  (None, 4, 64, 4)     0           ['normu1_2[0][0]']               
                                                                                                  
 convu1_3 (Conv2D)              (None, 4, 64, 4)     144         ['relu_7[0][0]']                 
                                                                                                  
 normu1_3 (BatchNormalization)  (None, 4, 64, 4)     16          ['convu1_3[0][0]']               
                                                                                                  
 relu_8 (ReLU)                  (None, 4, 64, 4)     0           ['normu1_3[0][0]']               
                                                                                                  
 out (Conv2D)                   (None, 4, 64, 1)     5           ['relu_8[0][0]']                 
                                                                                                  
 relu_9 (ReLU)                  (None, 4, 64, 1)     0           ['out[0][0]']                    
                                                                                                  
==================================================================================================
Total params: 1,978
Trainable params: 1,898
Non-trainable params: 80
__________________________________________________________________________________________________
None
Interpreting Model
Topology:
Layer name: pos_enc_main, layer type: InputLayer, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: input_images, layer type: InputLayer, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: emb1, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: add, layer type: Merge, input shapes: [[None, 4, 64, 1], [None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: convd1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 4]
Layer name: normd1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_1, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convd1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normd1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_2, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: pool3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 2, 32, 4]
Layer name: normb1_1, layer type: BatchNormalization, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: pos_enc_bottleneck, layer type: InputLayer, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: relu_3, layer type: Activation, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: emb4, layer type: Dense, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: add_1, layer type: Merge, input shapes: [[None, 2, 32, 4], [None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: convb1_1, layer type: Conv2D, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 8]
Layer name: normb1_2, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_4, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: convb1_2, layer type: Conv2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: normb1_3, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_5, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: up1, layer type: UpSampling2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 4, 64, 8]
Layer name: convu1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 8]], output shape: [None, 4, 64, 4]
Layer name: normu1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_6, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: emb5, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: add_2, layer type: Merge, input shapes: [[None, 4, 64, 4], [None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_7, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_3, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_8, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: out, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: relu_9, layer type: Activation, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
-----------------------------------
Configuration

-----------------------------------
Model               : {'Precision': 'ap_fixed<16, 6, AP_RND, AP_SAT>', 'ReuseFactor': 1, 'Strategy': 'Latency', 'BramFactor': 1000000000, 'TraceOutput': False, 'Pipeline': 'Dataflow'}
pos_enc_main        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
input_images        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb1                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb1_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normd1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_1              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normd1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_2              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pool3               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pool3_linear        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pos_enc_bottleneck  : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_3              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb4                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb4_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add_1               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_4              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_5              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
up1                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_6              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb5                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb5_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add_2               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_7              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_3_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_8              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
out                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
INFO:root:Loading .npy files from /storage/sa21722/stablediffusion/keras_version/signal.npy and /storage/sa21722/stablediffusion/keras_version/pile_up.npy
INFO:root:Re-sizing tensors...
2024-08-12 15:12:27.809390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8902
2024-08-12 15:12:27.994673: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
out_linear          : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_9              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
-----------------------------------

Interpreting Model
Topology:
Layer name: pos_enc_main, layer type: InputLayer, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: input_images, layer type: InputLayer, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: emb1, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: add, layer type: Merge, input shapes: [[None, 4, 64, 1], [None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: convd1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 4]
Layer name: normd1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_1, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convd1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normd1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_2, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: pool3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 2, 32, 4]
Layer name: normb1_1, layer type: BatchNormalization, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: pos_enc_bottleneck, layer type: InputLayer, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: relu_3, layer type: Activation, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: emb4, layer type: Dense, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: add_1, layer type: Merge, input shapes: [[None, 2, 32, 4], [None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: convb1_1, layer type: Conv2D, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 8]
Layer name: normb1_2, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_4, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: convb1_2, layer type: Conv2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: normb1_3, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_5, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: up1, layer type: UpSampling2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 4, 64, 8]
Layer name: convu1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 8]], output shape: [None, 4, 64, 4]
Layer name: normu1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_6, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: emb5, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: add_2, layer type: Merge, input shapes: [[None, 4, 64, 4], [None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_7, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_3, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_8, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: out, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: relu_9, layer type: Activation, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Creating HLS model
WARNING: Layer emb1 requires "dataflow" pipeline style. Switching to "dataflow" pipeline style.
WARNING: Config parameter "trace" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "precision" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "strategy" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "parallelization_factor" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "reuse_factor" overwrites an existing attribute in layer "out" (PointwiseConv2D)
Writing HLS project
Done
1/1 [==============================] - ETA: 0s1/1 [==============================] - 1s 1s/step
Done taking outputs for Keras model.
Recompiling myproject with tracing
Writing HLS project
Done
Plotting graphs for: emb1 layer
Plotting graphs for: add layer
Plotting graphs for: convd1_1 layer
Plotting graphs for: relu_1 layer
Plotting graphs for: convd1_2 layer
Plotting graphs for: relu_2 layer
Plotting graphs for: pool3 layer
Plotting graphs for: relu_3 layer
Plotting graphs for: emb4 layer
Plotting graphs for: add_1 layer
Plotting graphs for: convb1_1 layer
Plotting graphs for: relu_4 layer
Plotting graphs for: convb1_2 layer
Plotting graphs for: relu_5 layer
Plotting graphs for: up1 layer
Plotting graphs for: convu1_1 layer
Plotting graphs for: relu_6 layer
Plotting graphs for: emb5 layer
Plotting graphs for: add_2 layer
Plotting graphs for: convu1_2 layer
Plotting graphs for: relu_7 layer
Plotting graphs for: convu1_3 layer
Plotting graphs for: relu_8 layer
Plotting graphs for: out layer
Plotting graphs for: relu_9 layer
Interpreting Model
Topology:
Layer name: pos_enc_main, layer type: InputLayer, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: input_images, layer type: InputLayer, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: emb1, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: add, layer type: Merge, input shapes: [[None, 4, 64, 1], [None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Layer name: convd1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 4]
Layer name: normd1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_1, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convd1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normd1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_2, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: pool3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 2, 32, 4]
Layer name: normb1_1, layer type: BatchNormalization, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: pos_enc_bottleneck, layer type: InputLayer, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: relu_3, layer type: Activation, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: emb4, layer type: Dense, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: add_1, layer type: Merge, input shapes: [[None, 2, 32, 4], [None, 2, 32, 4]], output shape: [None, 2, 32, 4]
Layer name: convb1_1, layer type: Conv2D, input shapes: [[None, 2, 32, 4]], output shape: [None, 2, 32, 8]
Layer name: normb1_2, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_4, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: convb1_2, layer type: Conv2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: normb1_3, layer type: BatchNormalization, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: relu_5, layer type: Activation, input shapes: [[None, 2, 32, 8]], output shape: [None, 2, 32, 8]
Layer name: up1, layer type: UpSampling2D, input shapes: [[None, 2, 32, 8]], output shape: [None, 4, 64, 8]
Layer name: convu1_1, layer type: Conv2D, input shapes: [[None, 4, 64, 8]], output shape: [None, 4, 64, 4]
Layer name: normu1_1, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_6, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: emb5, layer type: Dense, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: add_2, layer type: Merge, input shapes: [[None, 4, 64, 4], [None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_2, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_2, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_7, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: convu1_3, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: normu1_3, layer type: BatchNormalization, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: relu_8, layer type: Activation, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 4]
Layer name: out, layer type: Conv2D, input shapes: [[None, 4, 64, 4]], output shape: [None, 4, 64, 1]
Layer name: relu_9, layer type: Activation, input shapes: [[None, 4, 64, 1]], output shape: [None, 4, 64, 1]
Creating HLS model
WARNING: Layer emb1 requires "dataflow" pipeline style. Switching to "dataflow" pipeline style.
WARNING: Config parameter "trace" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "precision" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "strategy" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "parallelization_factor" overwrites an existing attribute in layer "out" (PointwiseConv2D)
WARNING: Config parameter "reuse_factor" overwrites an existing attribute in layer "out" (PointwiseConv2D)
Profiling weights (before optimization)
Profiling weights (final / after optimization)
Profiling activations (before optimization)
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 449ms/step

****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source /software/CAD/Xilinx/2019.2/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace
INFO: Applying HLS Y2K22 patch v1.2 for IP revision
INFO: [HLS 200-10] Running '/software/CAD/Xilinx/2019.2/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'
INFO: [HLS 200-10] For user 'sa21722' on host 'gpu04.dice.priv' (Linux_x86_64 version 5.14.0-427.24.1.el9_4.x86_64) on Mon Aug 12 15:22:55 UTC 2024
INFO: [HLS 200-10] In directory '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel'
Sourcing Tcl script 'build_prj.tcl'
INFO: [HLS 200-10] Creating and opening project '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel/myproject_prj'.
INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project
INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project
INFO: [HLS 200-10] Creating and opening solution '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel/myproject_prj/solution1'.
INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.
INFO: [XFORM 203-1161] The maximum of name length is set into 80.
INFO: [HLS 200-10] Setting target device to 'xcvu13p-flga2577-2-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 2.778ns.
INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.347ns.
***** C SIMULATION *****
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
   Compiling ../../../../myproject_test.cpp in debug mode
   Compiling ../../../../firmware/myproject.cpp in debug mode
   Generating csim.exe
INFO: Unable to open input/predictions file, using default input.
0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.078125 0.078125 0.140625 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.0625 0.140625 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.21875 0.1875 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.0625 0.046875 0.1875 
INFO: Saved inference results to file: tb_data/csim_results.log
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
***** C SIMULATION COMPLETED IN 0h0m42s *****
***** C/RTL SYNTHESIS *****
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... 
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:59:86
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:59:91
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:73:72
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:73:76
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:87:75
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:87:80
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:101:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:101:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:115:94
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:115:99
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:129:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:129:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:143:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:143:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:164:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:164:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:178:86
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:178:91
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:192:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:192:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:206:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:206:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:220:87
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:220:92
WARNING: [HLS 200-471] Dataflow form checks found 24 issue(s) in file firmware/myproject.cpp
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:03:07 ; elapsed = 00:03:30 . Memory (MB): peak = 929.840 ; gain = 526.004 ; free physical = 340564 ; free virtual = 436109
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:03:07 ; elapsed = 00:03:30 . Memory (MB): peak = 929.840 ; gain = 526.004 ; free physical = 340564 ; free virtual = 436109
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 09:29:38 ; elapsed = 10:07:40 . Memory (MB): peak = 3666.875 ; gain = 3263.039 ; free physical = 349676 ; free virtual = 435363
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 09:52:26 ; elapsed = 10:31:08 . Memory (MB): peak = 3666.875 ; gain = 3263.039 ; free physical = 358323 ; free virtual = 443008
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config48>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config45>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config41>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config37>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config34>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' (firmware/nnet_utils/nnet_image.h:25:51).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config29>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config25>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config21>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config18>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config13>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config9>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config5>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' for pipelining.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config48>' completely with a factor of 256.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config45>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config41>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_merge.h:39) in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config37>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config34>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 72.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 72.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_image.h:27) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (firmware/nnet_utils/nnet_image.h:28) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1.1' (firmware/nnet_utils/nnet_image.h:31) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config29>' completely with a factor of 512.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' completely with a factor of 8.
ERROR: [XFORM 203-504] Stop unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' because it may cause large runtime and excessive memory usage due to increase in code size. Please avoid unrolling the loop or form sub-functions for code in the loop body.
ERROR: [HLS 200-70] Pre-synthesis failed.
command 'ap_source' returned error code
    while executing
"source build_prj.tcl"
    ("uplevel" body line 1)
    invoked from within
"uplevel \#0 [list source $arg] "

INFO: [Common 17-206] Exiting vivado_hls at Tue Aug 13 02:18:26 2024...
   emb1
   add
   convd1_1
   normd1_1
   relu_1
   convd1_2
   normd1_2
   relu_2
   pool3
   normb1_1
   relu_3
   emb4
   add_1
   convb1_1
   normb1_2
   relu_4
   convb1_2
   normb1_3
   relu_5
   up1
   convu1_1
   normu1_1
   relu_6
   emb5
   add_2
   convu1_2
   normu1_2
   relu_7
   convu1_3
   normu1_3
   relu_8
   out
   relu_9
Profiling activations (final / after optimization)
Recompiling myproject with tracing
Writing HLS project
Done
   emb1
   add
   convd1_1
   relu_1
   convd1_2
   relu_2
   pool3
   relu_3
   emb4
   add_1
   convb1_1
   relu_4
   convb1_2
   relu_5
   up1
   convu1_1
   relu_6
   emb5
   add_2
   convu1_2
   relu_7
   convu1_3
   relu_8
   out
   relu_9
BUILDING FIRMWARE
CSynthesis report not found.
Vivado synthesis report not found.
Cosim report not found.
Timing report not found.
pos_enc_main        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
input_images        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb1                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb1_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normd1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_1              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convd1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normd1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_2              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pool3               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pool3_linear        : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
pos_enc_bottleneck  : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_3              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb4                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb4_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add_1               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_4              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convb1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normb1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}

****** Vivado(TM) HLS - High-Level Synthesis from C, C++ and SystemC v2019.2 (64-bit)
  **** SW Build 2708876 on Wed Nov  6 21:39:14 MST 2019
  **** IP Build 2700528 on Thu Nov  7 00:09:20 MST 2019
    ** Copyright 1986-2019 Xilinx, Inc. All Rights Reserved.

source /software/CAD/Xilinx/2019.2/Vivado/2019.2/scripts/vivado_hls/hls.tcl -notrace
INFO: Applying HLS Y2K22 patch v1.2 for IP revision
INFO: [HLS 200-10] Running '/software/CAD/Xilinx/2019.2/Vivado/2019.2/bin/unwrapped/lnx64.o/vivado_hls'
INFO: [HLS 200-10] For user 'sa21722' on host 'gpu04.dice.priv' (Linux_x86_64 version 5.14.0-427.24.1.el9_4.x86_64) on Tue Aug 13 02:18:30 UTC 2024
INFO: [HLS 200-10] In directory '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel'
Sourcing Tcl script 'build_prj.tcl'
INFO: [HLS 200-10] Opening and resetting project '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel/myproject_prj'.
INFO: [HLS 200-10] Adding design file 'firmware/myproject.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'myproject_test.cpp' to the project
INFO: [HLS 200-10] Adding test bench file 'firmware/weights' to the project
INFO: [HLS 200-10] Adding test bench file 'tb_data' to the project
INFO: [HLS 200-10] Opening and resetting solution '/storage/sa21722/stablediffusion/keras_version/hls_outputs_ioparallel/myproject_prj/solution1'.
INFO: [HLS 200-10] Cleaning up the solution database.
INFO: [XFORM 203-101] Allowed max sub elements number after partition is 4096.
INFO: [XFORM 203-1161] The maximum of name length is set into 80.
INFO: [HLS 200-10] Setting target device to 'xcvu13p-flga2577-2-e'
INFO: [SYN 201-201] Setting up clock 'default' with a period of 2.778ns.
INFO: [SYN 201-201] Setting up clock 'default' with an uncertainty of 0.347ns.
***** C SIMULATION *****
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch GCC as the compiler.
   Compiling ../../../../myproject_test.cpp in debug mode
   Compiling ../../../../firmware/myproject.cpp in debug mode
   Generating csim.exe
INFO: Unable to open input/predictions file, using default input.
0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.078125 0.078125 0.140625 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.0625 0.140625 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.21875 0.1875 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.109375 0.0625 0.046875 0.1875 
INFO: Saved inference results to file: tb_data/csim_results.log
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
***** C SIMULATION COMPLETED IN 0h0m44s *****
***** C/RTL SYNTHESIS *****
INFO: [SCHED 204-61] Option 'relax_ii_for_timing' is enabled, will increase II to preserve clock frequency constraints.
INFO: [HLS 200-10] Analyzing design file 'firmware/myproject.cpp' ... 
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:59:86
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:59:91
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:73:72
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:73:76
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:87:75
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:87:80
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:101:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:101:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:115:94
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:115:99
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:129:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:129:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:143:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:143:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:164:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:164:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:178:86
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:178:91
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:192:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:192:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:206:77
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:206:82
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:220:87
WARNING: [HLS 214-113] Either use an argument of the function or declare the variable inside the dataflow loop body: firmware/myproject.cpp:220:92
WARNING: [HLS 200-471] Dataflow form checks found 24 issue(s) in file firmware/myproject.cpp
INFO: [HLS 200-111] Finished Linking Time (s): cpu = 00:02:48 ; elapsed = 00:03:07 . Memory (MB): peak = 929.840 ; gain = 526.004 ; free physical = 353760 ; free virtual = 438915
INFO: [HLS 200-111] Finished Checking Pragmas Time (s): cpu = 00:02:48 ; elapsed = 00:03:07 . Memory (MB): peak = 929.840 ; gain = 526.004 ; free physical = 353764 ; free virtual = 438919
INFO: [HLS 200-10] Starting code transformations ...
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'void nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>(FORWARD_REFERENCE*, FORWARD_REFERENCE*, FORWARD_REFERENCE::weight_t*, FORWARD_REFERENCE::bias_t*)' completely with a factor of 1.
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::product::mult<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0> >::product' into 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' (firmware/nnet_utils/nnet_conv2d_latency.h:55).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' into 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' (firmware/nnet_utils/nnet_conv2d.h:50).
INFO: [XFORM 203-603] Inlining function 'nnet::conv_2d_latency_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' into 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' (firmware/nnet_utils/nnet_conv2d.h:67).
INFO: [HLS 200-111] Finished Standard Transforms Time (s): cpu = 09:06:31 ; elapsed = 09:36:28 . Memory (MB): peak = 3678.906 ; gain = 3275.070 ; free physical = 357531 ; free virtual = 442847
INFO: [HLS 200-10] Checking synthesizability ...
INFO: [HLS 200-111] Finished Checking Synthesizability Time (s): cpu = 09:31:58 ; elapsed = 10:03:04 . Memory (MB): peak = 3678.906 ; gain = 3275.070 ; free physical = 357776 ; free virtual = 443107
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config48>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config45>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config41>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config37>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config34>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' (firmware/nnet_utils/nnet_image.h:25:51).
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config29>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config25>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config22>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config21>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config50>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config18>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config14>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config13>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config10>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config9>' (firmware/nnet_utils/nnet_activation.h:40:43).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config6>' for pipelining.
INFO: [XFORM 203-502] Unrolling all loops for pipelining in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config5>' (firmware/nnet_utils/nnet_merge.h:37:19).
INFO: [XFORM 203-502] Unrolling all sub-loops inside loop 'PartitionLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:35) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config49>' for pipelining.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config48>' completely with a factor of 256.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config52>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config45>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config42>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config41>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 36.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config38>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_merge.h:39) in function 'nnet::add<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config37>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::pointwise_conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config51>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config34>' completely with a factor of 1024.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Product1' (firmware/nnet_utils/nnet_conv2d_latency.h:48) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 72.
INFO: [HLS 200-489] Unrolling loop 'Product2' (firmware/nnet_utils/nnet_conv2d_latency.h:52) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'ResetAccum' (firmware/nnet_utils/nnet_conv2d_latency.h:62) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Accum1' (firmware/nnet_utils/nnet_conv2d_latency.h:69) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 72.
INFO: [HLS 200-489] Unrolling loop 'Accum2' (firmware/nnet_utils/nnet_conv2d_latency.h:72) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Result' (firmware/nnet_utils/nnet_conv2d_latency.h:80) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config31>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_image.h:27) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 4.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1' (firmware/nnet_utils/nnet_image.h:28) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 64.
INFO: [HLS 200-489] Unrolling loop 'Loop-1.1.1' (firmware/nnet_utils/nnet_image.h:31) in function 'nnet::resize_nearest<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config30>' completely with a factor of 8.
INFO: [HLS 200-489] Unrolling loop 'Loop-1' (firmware/nnet_utils/nnet_activation.h:43) in function 'nnet::relu<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ReLU_config29>' completely with a factor of 512.
INFO: [HLS 200-489] Unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' completely with a factor of 8.
ERROR: [XFORM 203-504] Stop unrolling loop 'PixelLoop' (firmware/nnet_utils/nnet_conv2d_latency.h:41) in function 'nnet::conv_2d_cl<ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, ap_fixed<8, 2, (ap_q_mode)0, (ap_o_mode)0, 0>, config26>' because it may cause large runtime and excessive memory usage due to increase in code size. Please avoid unrolling the loop or form sub-functions for code in the loop body.
ERROR: [HLS 200-70] Pre-synthesis failed.
command 'ap_source' returned error code
    while executing
"source build_prj.tcl"
    ("uplevel" body line 1)
    invoked from within
"uplevel \#0 [list source $arg] "

INFO: [Common 17-206] Exiting vivado_hls at Tue Aug 13 12:47:00 2024...
hls_converter.py:238: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure(facecolor='w')
relu_5              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
up1                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_1_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_1            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_6              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb5                : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
emb5_linear         : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
add_2               : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_2_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_2            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_7              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
convu1_3_linear     : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
normu1_3            : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'scale': 'fixed<16,6>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_8              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
out                 : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
out_linear          : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
relu_9              : {'Trace': True, 'Precision': {'result': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'weight': 'ap_fixed<8, 2, AP_RND, AP_SAT>', 'bias': 'ap_fixed<8, 2, AP_RND, AP_SAT>'}, 'Strategy': 'Latency', 'ParallelizationFactor': 8, 'ReuseFactor': 1}
-----------------------------------
------ HLS SYNTHESIS COMPLETE -----
